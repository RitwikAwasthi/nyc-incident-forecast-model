{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NYC Incident Prediction Model",
   "id": "a47f4efee18312da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "id": "92d35ba0370d1919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import  RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import root_mean_squared_error"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions",
   "id": "a2ac7b35d0f5ca82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_timeseries(df, cols):\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=len(cols), figsize=(25,5))\n",
    "    for i, col in enumerate(cols):\n",
    "        sp = ax[i].scatter(\n",
    "                df[col[0]],\n",
    "                df[col[1]],\n",
    "                c=df[\"target\"]\n",
    "             )\n",
    "        ax[i].set_title(col[2])\n",
    "    _ = fig.colorbar(sp)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_3d_timeseries(df, graphs):\n",
    "    for graph in graphs:\n",
    "        cols, title = graph[0], graph[1]\n",
    "        scatter_plot = px.scatter_3d(df, x=cols[0], y=cols[1], z=cols[2], color='target', title=title, range_color=[0,200], height=600, width=600)\n",
    "        scatter_plot.show()"
   ],
   "id": "46d0301f1993bac1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "3cab831d6ae21528"
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../data/encoded_df.csv')\n",
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89eb19cc65acd3ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualizing Time Encoded Data",
   "id": "4babe3078de81c86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Two-dimensional representation of cyclic encoding",
   "id": "a62f12adf8deb352"
  },
  {
   "cell_type": "code",
   "source": [
    "twoDim_graphs =  [['month_cos', 'month_sin','Month Of Year'], ['day_cos', 'day_sin','Day Of Month'], ['hour_cos', 'hour_sin','Hour Of Day'], ['yearday_cos', 'yearday_sin','Day Of Year'], ['weekday_cos', 'weekday_sin','Day Of Week']]\n",
    "plot_timeseries(df, twoDim_graphs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32aa78aba71ba8b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Three-dimensional representation of cyclic encoding",
   "id": "3c3f236faafe0aa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "threeDim_graphs = [\n",
    "    [['day_cos', 'day_sin', 'month_of_year'], 'Target vs. Days of Month'], \n",
    "    [['hour_cos', 'hour_sin', 'day_of_year'], 'Target vs. Hourly Day of Year'], \n",
    "    [['hour_cos', 'hour_sin', 'day_of_week'], 'Target vs. Hourly Day of Week'],\n",
    "    [['hour_cos', 'hour_sin', 'month_of_year'], 'Target vs. Hourly Month of Year'],\n",
    "    [['month_cos', 'month_sin', 'day_of_year'], 'Target by Monthly Day of Year']\n",
    "]\n",
    "plot_3d_timeseries(df, threeDim_graphs)"
   ],
   "id": "d5c449f49c22a24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Selection"
   ],
   "id": "14e4af258022747e"
  },
  {
   "cell_type": "code",
   "source": [
    "# drop columns containing pre-encoded time data\n",
    "original_time_cols = ['year', 'month_of_year', 'day_of_month', 'hour_of_day', 'day_of_week', 'day_of_year']\n",
    "df = df.drop(original_time_cols, axis=1)\n",
    "\n",
    "# convert columns to categorical\n",
    "categorical_cols = ['is_holiday', 'is_leap_year', 'BORO_NM']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "# fix categorical conversion issue\n",
    "df['BORO_NM_STATEN_ISLAND'] = df['BORO_NM_STATEN ISLAND']\n",
    "df = df.drop(['BORO_NM_STATEN ISLAND'], axis=1)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size= 0.20)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36e5720a732a166d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Basic Model Selection*:\n",
    "\n",
    "- Linear Regression\n",
    "- KNN Regression\n",
    "- Random Forest Regression\n",
    "- Gradient Boosting Regression"
   ],
   "id": "d695c3099fb22f1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clf0 = LinearRegression()\n",
    "clf1 = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "clf2 = RandomForestRegressor(n_jobs=-1, oob_score=True, random_state=1122)\n",
    "clf3 = GradientBoostingRegressor(n_estimators=10)\n",
    "\n",
    "models = [clf0, clf1, clf2, clf3]\n",
    "\n",
    "# training data metrics\n",
    "print(f\"\\n --- Training Data Statistics ---- \\n\")\n",
    "print(f'Train Mean: {np.round(y_test.mean(), 2)}')\n",
    "print(f'Train Median: {np.round(y_test.median(), 2)}')\n",
    "print(f'Train Std Dev: {np.round(y_test.std(), 2)}')\n",
    "\n",
    "# model metrics\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"\\n --- Model {i+1} ---- \\n\")\n",
    "    res = model.fit(X_train, y_train)\n",
    "    y_pred = res.predict(X_test)\n",
    "    y_pred_train = res.predict(X_train)\n",
    "    print(f'RMSE Train: {np.round(root_mean_squared_error(y_pred= y_pred_train, y_true= y_train), 2)}')\n",
    "    print(f'RMSE Test: {np.round(root_mean_squared_error(y_pred= y_pred, y_true= y_test), 2)}')"
   ],
   "id": "bd62f693f2ea2068",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "While Gradient Boosting appears loose to Random Forest, we can see that the random forest model is overfitting. We must also consider that gradient boosting is very sensitive to hyperparameter tuning.\n",
    "\n",
    "I will proceed with the Gradient Boosted model as I believe that it's performance can match *(if not surpass)* the performance of the random forest model while having better control over the model overfitting."
   ],
   "id": "d0ccce2688fad8c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gradient Boosting Model Tuning"
   ],
   "id": "63f64e39ba4e524d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In theory, we need to limit the depth of trees created by the gradient boosting model in order to force them into being weak learners. The core idea behind gradient boosting requires that our model iteratively creates weak learners **(stumps)** that eventually mesh together to form a strong predictor.\n",
    "\n",
    "We will utilize GridSearchCV in order to efficiently search the parameter space.\n",
    "\n",
    "Parameter Search Space:\n",
    "\n",
    "- Max Depth: [1, 2, 3]\n",
    "- Number of Estimators: [100, 200, 500, 1000, 2000]\n",
    "- Learning Rate: [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "- Minimum Impurity Decrease: [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "- Max Features: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"
   ],
   "id": "905cfb292bbac8a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# grid search cv\n",
    "n_estimators = [10, 50, 100, 200, 500]\n",
    "param_grid = [{\"max_depth\": [1, 2, 3], 'n_estimators': [100, 200, 500, 1000, 2000], 'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'min_impurity_decrease': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'max_features': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}]\n",
    "\n",
    "gs5 = GridSearchCV(GradientBoostingRegressor(random_state=1122), param_grid=param_grid, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "gs5.fit(X_train, y_train)\n",
    "\n",
    "scores = gs5.cv_results_['mean_test_score']*-1\n",
    "print(gs5.best_estimator_)"
   ],
   "id": "2fa5e8fd8caab91a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Results of Grid Search CV:\n",
    "\n",
    "- Max Depth: 3\n",
    "- Learning Rate: 0.475\n",
    "- Minimum Impurity Decrease: 0.175\n",
    "- Number of Estimators: 500\n",
    "- Max Features: 1.0"
   ],
   "id": "a2cf8190dd2a8a9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Computational Performance"
   ],
   "id": "fafe493302bb1842"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*At what point do we stop creating estimators? At what point is it not worth the computation to predict 1 more incident correctly?*\n",
    "\n",
    "We must consider the performance of our model in situations where it would be applied. Creating more estimators does not always benefit us as it affects the time complexity of the model."
   ],
   "id": "cbbddc6a13332031"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plotting effect of n_estimators on Train and Test RMSE\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "n_estimators = [10, 20, 30, 50, 100, 200, 500, 1000]\n",
    "\n",
    "for i in n_estimators:\n",
    "    champion_gb = GradientBoostingRegressor(random_state=1122, learning_rate=0.475, min_impurity_decrease=0.175, n_estimators=i, max_depth=3, max_features=1.0).fit(X_train, y_train)\n",
    "    cgb_res = champion_gb.predict(X_test)\n",
    "    cgb_res_t = champion_gb.predict(X_train)\n",
    "    train_scores.append(np.round(root_mean_squared_error(y_pred=cgb_res_t, y_true= y_train), 2))\n",
    "    test_scores.append(np.round(root_mean_squared_error(y_pred=cgb_res, y_true= y_test), 2))\n",
    "    \n",
    "plt.plot(n_estimators, train_scores, label='Train', c='Blue')\n",
    "plt.plot(n_estimators, test_scores, label='Test', c='Red')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Estimators\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"GB:\\nTrain RMSE vs. Test RMSE\")\n",
    "plt.show()"
   ],
   "id": "cc724b1d90737b62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Champion Model Evaluation",
   "id": "c042b05ec56c35c9"
  },
  {
   "cell_type": "code",
   "source": [
    "champion_gb = GradientBoostingRegressor(random_state=1122, learning_rate=0.475, min_impurity_decrease=0.175, n_estimators=200, max_depth=3, max_features=1.0).fit(X_train, y_train)\n",
    "cgb_res = champion_gb.predict(X_test)\n",
    "print(f'Champion GB RMSE: {np.round(root_mean_squared_error(y_pred=cgb_res, y_true= y_test), 2)}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d34654015922b95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pack and Publish",
   "id": "6ad2b51e1a099ee0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "joblib.dump(value=[champion_gb, list(X_train.columns), y_train], filename='../data/model.pkl')"
   ],
   "id": "eb9fdd152909f152",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
